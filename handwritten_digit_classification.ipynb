{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09c0e010",
   "metadata": {},
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c654121c-26d7-421c-a998-503343e34a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "import cv2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import SGD, Adam\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from torchmetrics import F1Score\n",
    "\n",
    "from torchvision import  datasets, transforms\n",
    "import torchvision.models as models\n",
    "\n",
    "import tqdm\n",
    "\n",
    "\n",
    "torch.set_default_dtype(torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae444f2-aa69-4b6d-b44c-99f39cbe1f32",
   "metadata": {},
   "source": [
    "# splitting a dataset into training, validation, and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bc8047b1-3f2d-46d9-82ca-47c2080faa02",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = './handwritten'\n",
    "split_dir = './ttv'\n",
    "train_dir = train_data_path = f'{split_dir}/train'\n",
    "val_dir = val_data_path = f'{split_dir}/val'\n",
    "test_dir = test_data_path = f'{split_dir}/test'\n",
    "\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "os.makedirs(val_dir, exist_ok=True)\n",
    "os.makedirs(test_dir, exist_ok=True)\n",
    "\n",
    "if not os.path.isdir(split_dir):\n",
    "    for class_folder in os.listdir(data_dir):\n",
    "        class_path = os.path.join(data_dir, class_folder)\n",
    "        if not os.path.isdir(class_path):\n",
    "            continue\n",
    "\n",
    "        # Create class folders in train, validation, and test directories\n",
    "        train_class_path = os.path.join(train_dir, class_folder)\n",
    "        val_class_path = os.path.join(val_dir, class_folder)\n",
    "        test_class_path = os.path.join(test_dir, class_folder)\n",
    "        os.makedirs(train_class_path, exist_ok=True)\n",
    "        os.makedirs(val_class_path, exist_ok=True)\n",
    "        os.makedirs(test_class_path, exist_ok=True)\n",
    "\n",
    "        # List all images in the class folder\n",
    "        images = os.listdir(class_path)\n",
    "\n",
    "        # Shuffle the images\n",
    "        random.shuffle(images)\n",
    "\n",
    "        # Split the images into train, validation, and test sets (e.g., 70-20-10 split)\n",
    "        train_split_index = int(0.7 * len(images))\n",
    "        val_split_index = int(0.9 * len(images))\n",
    "        train_images = images[:train_split_index]\n",
    "        val_images = images[train_split_index:val_split_index]\n",
    "        test_images = images[val_split_index:]\n",
    "\n",
    "        # Move images to train, validation, and test folders\n",
    "        for image in train_images:\n",
    "            src = os.path.join(class_path, image)\n",
    "            dst = os.path.join(train_class_path, image)\n",
    "            shutil.copy(src, dst)\n",
    "\n",
    "        for image in val_images:\n",
    "            src = os.path.join(class_path, image)\n",
    "            dst = os.path.join(val_class_path, image)\n",
    "            shutil.copy(src, dst)\n",
    "\n",
    "        for image in test_images:\n",
    "            src = os.path.join(class_path, image)\n",
    "            dst = os.path.join(test_class_path, image)\n",
    "            shutil.copy(src, dst)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c916f0d7-4292-4bd6-82d5-2c7fd551f609",
   "metadata": {},
   "source": [
    "# test some transformations, gain visual insight into the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b1e9fc",
   "metadata": {},
   "source": [
    "##### Define Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e8df3f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.RandomRotation(degrees=25),  # Rotate the image randomly between -45 and 45 degrees\n",
    "    #transforms.RandomHorizontalFlip(p=0.5),  # Randomly flip the image horizontally with a probability of 0.5\n",
    "    transforms.RandomResizedCrop(size=224, scale=(0.8, 1.0), ratio=(0.75, 1.333)),  # Randomly crop and resize the image\n",
    "    transforms.ToTensor(),  # Convert the image to a PyTorch tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize the image with mean and standard deviation\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b78677",
   "metadata": {},
   "source": [
    "##### Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1c95cf93",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = datasets.ImageFolder(train_data_path, transform=transform)\n",
    "val_dataset = datasets.ImageFolder(val_data_path, transform=transform)\n",
    "test_dataset = datasets.ImageFolder(test_data_path, transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144e7125",
   "metadata": {},
   "source": [
    "##### Create DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "64982589",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908d75d6",
   "metadata": {},
   "source": [
    "##### Dataset sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31c7d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training dataset size: ', len(train_dataset))\n",
    "print('Validation dataset size: ', len(val_dataset))\n",
    "print('Test dataset size: ', len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7757bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over DataLoader\n",
    "for images, labels in train_loader:\n",
    "    print(images.shape)\n",
    "    print(labels.shape)\n",
    "    break  # break after printing the first batch\n",
    "\n",
    "# Display one image\n",
    "plt.imshow(images[0].numpy().squeeze().transpose(1, 2, 0), cmap='gray_r')\n",
    "plt.title(f'Label: {labels[0]}')\n",
    "plt.show()\n",
    "\n",
    "# Display multiple images\n",
    "figure = plt.figure(figsize=(15, 10))\n",
    "num_of_images = 60\n",
    "for index in range(1, num_of_images + 1):\n",
    "    plt.subplot(6, 10, index)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(images[index].numpy().squeeze().transpose(1, 2, 0), cmap='gray_r')\n",
    "    plt.title(f'Label: {labels[index]}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ca1548",
   "metadata": {},
   "source": [
    "# Model Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c271bb05",
   "metadata": {},
   "source": [
    "##### CustomDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3a349da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class custom_dataset(Dataset):\n",
    "    def __init__(self, mode = 'train', root = f'{split_dir}/', transforms = None):\n",
    "        super().__init__()\n",
    "        self.mode = mode\n",
    "        self.root = root\n",
    "        self.transforms = transforms\n",
    "        \n",
    "        #select split\n",
    "        self.folder = os.path.join(self.root, self.mode)\n",
    "        \n",
    "        #initialize lists\n",
    "        self.image_list = []\n",
    "        self.label_list = []\n",
    "        \n",
    "        #save class lists\n",
    "        self.class_list = os.listdir(self.folder)\n",
    "        self.class_list.sort()\n",
    "        \n",
    "        for class_id in range(len(self.class_list)):\n",
    "            for image in os.listdir(os.path.join(self.folder, self.class_list[class_id])):\n",
    "                self.image_list.append(os.path.join(self.folder, self.class_list[class_id], image))\n",
    "                label = np.zeros(len(self.class_list), dtype=np.float32)\n",
    "                label[class_id] = 1.0\n",
    "                self.label_list.append(label)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_name = self.image_list[index]\n",
    "        label = self.label_list[index]\n",
    "        \n",
    "        \n",
    "        image = Image.open(image_name)\n",
    "        if(self.transforms):\n",
    "            image = self.transforms(image)\n",
    "        \n",
    "        label = torch.tensor(label)\n",
    "        \n",
    "        return image, label\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.image_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda9065b",
   "metadata": {},
   "source": [
    "### ResNet18 models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a5e5fb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet18(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.resnet18 = models.resnet18(weights=None)\n",
    "        self.resnet18 = torch.nn.Sequential(*(list(self.resnet18.children())[:-1]))\n",
    "        self.classifier = nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, image):\n",
    "        # Get predictions from ResNet18\n",
    "        resnet_pred = self.resnet18(image).squeeze()\n",
    "        out = self.classifier(resnet_pred)\n",
    "        return out\n",
    "\n",
    "# pretrained one\n",
    "class ResNet18_Pre(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.resnet18 = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)  # Load pretrained ResNet18\n",
    "        # Modify the model by removing the final fully connected layer\n",
    "        self.resnet18 = torch.nn.Sequential(*(list(self.resnet18.children())[:-1]))\n",
    "        # Freeze the layers of ResNet18\n",
    "        for param in self.resnet18.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        self.classifier = nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, image):\n",
    "        # Get predictions from ResNet18 and remove the extra dimension\n",
    "        resnet_pred = self.resnet18(image).squeeze()  # Squeeze to remove the extra dimension\n",
    "        out = self.classifier(resnet_pred)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d406df",
   "metadata": {},
   "source": [
    "### VGG16 Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "21fa53bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class VGG16(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.vgg16 = models.vgg16(weights=None)\n",
    "\n",
    "        self.vgg16.classifier = torch.nn.Sequential(*(list(self.vgg16.classifier.children())[:-1]))\n",
    "        # Define the new classifier\n",
    "        self.classifier = nn.Linear(4096, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass input through the VGG16 backbone\n",
    "        x = self.vgg16(x)\n",
    "        # Flatten the output\n",
    "        x = x.view(x.size(0), -1)\n",
    "        # Pass through the new classifier\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "#pretrained one\n",
    "class VGG16_Pre(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__() \n",
    "        self.vgg16 = models.vgg16(weights=models.VGG16_Weights.DEFAULT)\n",
    "        \n",
    "        for param in self.vgg16.features.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.vgg16.classifier = torch.nn.Sequential(*(list(self.vgg16.classifier.children())[:-1]))\n",
    "\n",
    "        # Define the new classifier\n",
    "        self.classifier = nn.Linear(4096, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass input through the VGG16 backbone\n",
    "        x = self.vgg16(x)\n",
    "        # Flatten the output\n",
    "        x = x.view(x.size(0), -1)\n",
    "        # Pass through the new classifier\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "01b378b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model_path = \"checkpoints/\"\n",
    "pth_name = \"saved_model.pth\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0091c05",
   "metadata": {},
   "source": [
    "# Custom Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc965fbb",
   "metadata": {},
   "source": [
    "### Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9ba5c588",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, data_val, loss_function, writer, epoch, device):\n",
    "    f1score = 0\n",
    "    f1 = F1Score(num_classes=len(data_val.dataset.class_list), task = 'multiclass')\n",
    "    data_iterator = enumerate(data_val)  # take batches\n",
    "    f1_list = []\n",
    "    f1t_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()  # switch model to evaluation mode\n",
    "        tq = tqdm.tqdm(total=len(data_val))\n",
    "        tq.set_description('Validation:')\n",
    "\n",
    "        total_loss = 0\n",
    "\n",
    "        for _, batch in data_iterator:\n",
    "            # forward propagation\n",
    "            image, label = batch\n",
    "            image = image.to(device)\n",
    "            label = label.to(device)\n",
    "            pred = model(image)\n",
    "\n",
    "            loss = loss_function(pred, label.float())\n",
    "\n",
    "            pred = pred.softmax(dim=1)\n",
    "            \n",
    "            f1_list.extend(torch.argmax(pred, dim =1).tolist())\n",
    "            f1t_list.extend(torch.argmax(label, dim =1).tolist())\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            tq.update(1)  \n",
    "    f1score = f1(torch.tensor(f1_list), torch.tensor(f1t_list))\n",
    "    writer.add_scalar(\"Validation F1\", f1score, epoch)\n",
    "    writer.add_scalar(\"Validation Loss\", total_loss/len(data_val), epoch)\n",
    "    tq.close()\n",
    "    print(\"F1 score: \", f1score)\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4dda72f",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "dc21b134",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, optimizer, loss_fn, n_epochs, device, writer,pth_n=pth_name):\n",
    "    model.to(device)  # Move the model to the specified device (e.g., GPU or CPU)\n",
    "    model.train()  # Set the model to training mode\n",
    "    for epoch in range(n_epochs):\n",
    "\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        tq = tqdm.tqdm(total=len(train_loader))\n",
    "        tq.set_description('epoch %d' % (epoch))\n",
    "\n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "            images = images.to(device)  # Move the batch of images to the specified device\n",
    "            labels = labels.to(device)  # Move the batch of labels to the specified device\n",
    "\n",
    "            optimizer.zero_grad()  # Reset the gradients of the optimizer\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            # Compute loss\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            # Update model parameters\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            tq.set_postfix(loss_st='%.6f' % loss.item())\n",
    "            tq.update(1)\n",
    "\n",
    "        print(\"Training Loss\", running_loss / len(train_loader))\n",
    "        writer.add_scalar(\"Training Loss\", running_loss / len(train_loader), epoch)\n",
    "        tq.close()\n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch + 1, n_epochs, epoch_loss))\n",
    "        # check the performance of the model on unseen dataset\n",
    "        validate(model, val_loader, loss_fn, writer, epoch, device)\n",
    "\n",
    "        # save the model in pth format\n",
    "        checkpoint = {\n",
    "            'epoch': epoch + 1,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict()\n",
    "        }\n",
    "\n",
    "        torch.save(checkpoint, os.path.join(save_model_path, pth_n))\n",
    "        print(\"saved the model \" + save_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e9a3cc",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "25e07ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model_name, Model, test_loader):\n",
    "    print(f'Model - {model_name.split('/')[-1]}:')\n",
    "    print('Testing...')\n",
    "\n",
    "    # Load the state dict from the checkpoint file\n",
    "    checkpoint = torch.load(model_name, weights_only=True)\n",
    "\n",
    "    # Extract only the model state_dict from the checkpoint\n",
    "    model_state_dict = checkpoint['state_dict']\n",
    "\n",
    "    # Initialize the model\n",
    "    model = Model(num_classes=len(train_data.class_list))\n",
    "\n",
    "    # Load the model weights into the model\n",
    "    model.load_state_dict(model_state_dict)\n",
    "\n",
    "    # Move the model to the appropriate device (e.g., 'cpu' or 'cuda')\n",
    "    device = torch.device('cuda')\n",
    "    model.to(device)\n",
    "\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    f1 = F1Score(num_classes=len(test_loader.dataset.class_list), average='macro', task='multiclass').to(device)\n",
    "    class_correct = torch.zeros(len(test_loader.dataset.class_list), device=device)\n",
    "    class_total = torch.zeros(len(test_loader.dataset.class_list), device=device)        \n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            _, labels = torch.max(labels, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            # Update class-wise accuracy\n",
    "            for i in range(len(labels)):\n",
    "                label = labels[i]\n",
    "                class_correct[label] += (predicted[i] == label).item()\n",
    "                class_total[label] += 1\n",
    "                \n",
    "            # Update F1 score\n",
    "            f1.update(predicted, labels)\n",
    "    accuracy = 100 * correct / total\n",
    "    print('Overall accuracy of the network on the test images: %.3f %%' % accuracy)\n",
    "    \n",
    "    f1_score = f1.compute()\n",
    "    print('F1 score of the network on the test images: %.3f\\n' % f1_score)\n",
    "\n",
    "    return accuracy, f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e9ce589a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformations\n",
    "train_tr = transforms.Compose([\n",
    "    transforms.RandomRotation(degrees=25),\n",
    "    transforms.Resize([224, 224]),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "val_test_tr = transforms.Compose([\n",
    "    transforms.Resize([224, 224]),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Load datasets\n",
    "train_data = custom_dataset(mode=\"train\", transforms=train_tr)\n",
    "val_data = custom_dataset(mode=\"val\", transforms=val_test_tr)\n",
    "test_data = custom_dataset(mode=\"test\", transforms=val_test_tr)\n",
    "\n",
    "# Define data loaders\n",
    "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=16, drop_last=True)\n",
    "test_loader = DataLoader(test_data, batch_size=16, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25f8201",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "66fddf04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model_name, Model, train_data, Optimizer, epochs, writer_name, lr, momentum=0):\n",
    "    # Choose optimizer and loss function\n",
    "    model = Model(num_classes=len(train_data.class_list))\n",
    "    optimizer = Optimizer(model.parameters(), lr=lr)\n",
    "\n",
    "    # Setup TensorBoard\n",
    "    writer = SummaryWriter(os.path.join('runs', writer_name))\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    train(model, train_loader, val_loader, optimizer, loss_fn, n_epochs=epochs, device='cuda', writer=writer, pth_n=model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3686d3e",
   "metadata": {},
   "source": [
    "##### ResNet18 Not Pretrained SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bbe9ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model('ResNet18_sgd_not_pre', ResNet18, train_data, SGD, 20, writer_name='resnet-not-pre-sgd', lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b253d258",
   "metadata": {},
   "source": [
    "##### ResNet18 Pretrained SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bfcd39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model('ResNet18_sgd_pre', ResNet18_Pre, train_data, SGD, 20, writer_name='resnet-pre-sgd', lr=0.0001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5155cb82",
   "metadata": {},
   "source": [
    "##### ResNet18 Not Pretrained Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226c34a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model('ResNet18_adam_not_pre', ResNet18, train_data, Adam, 20, writer_name='resnet-not-pre-adam', lr=0.00001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7050c8cb",
   "metadata": {},
   "source": [
    "##### ResNet18 Pretrained Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35aa88c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model('ResNet18_adam_pre', ResNet18_Pre, train_data, Adam, 20, writer_name='resnet-pre-adam', lr=0.00075)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5859d8",
   "metadata": {},
   "source": [
    "##### VGG16 Not Pretrained SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f0f76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model('VGG16_sgd_not_pre', VGG16, train_data, SGD, 10, writer_name='vgg-not-pre-sgd'lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d062fc1e",
   "metadata": {},
   "source": [
    "##### VGG16 Pretrained SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48baac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model('VGG16_sgd_pre', VGG16_Pre, train_data, SGD, 10, writer_name='vgg-pre-sgd', lr=0.00025, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf2c0f9",
   "metadata": {},
   "source": [
    "##### VGG16 Not Pretrained Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71cf0d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model('VGG16_adam_not_pre', VGG16, train_data, Adam, 10, writer_name='vgg-not-pre-adam', lr=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484392ca",
   "metadata": {},
   "source": [
    "##### VGG16 Pretrained Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89fe15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model('VGG16_adam_pre', VGG16_Pre, train_data, Adam, 10, writer_name='vgg-pre-adam', lr=0.00001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04afbcfd",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be638e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate('./checkpoints/ResNet18_sgd_not_pre', ResNet18, test_loader)\n",
    "evaluate('./checkpoints/ResNet18_sgd_pre', ResNet18_Pre, test_loader)\n",
    "evaluate('./checkpoints/ResNet18_adam_not_pre', ResNet18, test_loader)\n",
    "evaluate('./checkpoints/ResNet18_adam_pre', ResNet18_Pre, test_loader)\n",
    "evaluate('./checkpoints/VGG16_sgd_not_pre', VGG16, test_loader)\n",
    "evaluate('./checkpoints/VGG16_sgd_pre', VGG16_Pre, test_loader)\n",
    "evaluate('./checkpoints/VGG16_adam_not_pre', VGG16, test_loader)\n",
    "evaluate('./checkpoints/VGG16_adam_pre', VGG16_Pre, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8117f5",
   "metadata": {},
   "source": [
    "# Real-Time Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e162f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name, Model = './checkpoints/VGG16_sgd_not_pre', VGG16\n",
    "\n",
    "# Load the trained model\n",
    "checkpoint = torch.load(model_name, map_location='cuda', weights_only=True)  # Load for Apple Silicon\n",
    "\n",
    "# Extract only the model state_dict from the checkpoint\n",
    "model_state_dict = checkpoint['state_dict']\n",
    "\n",
    "# Initialize the model\n",
    "model = Model(num_classes=len(train_data.class_list))\n",
    "\n",
    "# Load the model weights into the model\n",
    "model.load_state_dict(model_state_dict)\n",
    "\n",
    "# Move the model to the appropriate device (e.g., 'cpu' or 'cuda')\n",
    "device = torch.device('cuda')\n",
    "model.to(device)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Initialize the webcam\n",
    "cap = cv2.VideoCapture(1)  # Use 0 for the default camera\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Convert BGR (OpenCV default) to RGB for PIL compatibility\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Preprocess the frame\n",
    "    resized_frame = cv2.resize(frame_rgb, (224, 224))  # Resize for the model\n",
    "    pil_image = Image.fromarray(resized_frame)  # Convert NumPy array to PIL Image\n",
    "    tensor_frame = transform(pil_image).unsqueeze(0).to(\"cuda\")  # Add batch dim and move to cuda\n",
    "\n",
    "    # Predict using the model\n",
    "    with torch.no_grad():\n",
    "        output = model(tensor_frame)\n",
    "        print(output.shape)  # Debugging step: Inspect the output shape\n",
    "        if output.dim() == 1:  # Case 1: Single instance with no batch dimension\n",
    "            predicted_class = output.argmax(dim=0).item()\n",
    "        elif output.dim() == 2:  # Case 2: Batch dimension present\n",
    "            predicted_class = output.argmax(dim=1).item()\n",
    "        else:\n",
    "            raise ValueError(f'Unexpected output shape: {output.shape}')\n",
    "\n",
    "    # Display the result\n",
    "    cv2.putText(frame, f'Prediction: {predicted_class}', (10, 90),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 3, (255, 0, 255), 6, cv2.LINE_AA)\n",
    "    cv2.imshow('Number Recognition', frame)\n",
    "\n",
    "    # Break the loop on 'q' key press\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
